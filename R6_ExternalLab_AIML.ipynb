{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_ExternalLab_AIML.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YYk8NG3yOIT9"
      },
      "source": [
        "### A MNIST-like fashion product database\n",
        "\n",
        "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tFO6PuxzOIT_"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "efNjNImfOIUC",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.set_random_seed(42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9C4aAIGOIUH",
        "outputId": "b86ad392-a02c-4b72-eb8c-cba5caa10789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcoZBStrOIUQ"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XA1WsFSeOIUS",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qnbx7TyQOIUY",
        "colab": {}
      },
      "source": [
        "(trainX, trainY), (testX, testY) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UbiHj5YPOIUc",
        "outputId": "a4a2eb7a-bdff-4dec-86cb-4c339ba87714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(testY[0:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lDAYzkwyOIUj"
      },
      "source": [
        "### Convert both training and testing labels into one-hot vectors.\n",
        "\n",
        "**Hint:** check **tf.keras.utils.to_categorical()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vBlfYlANOIUk",
        "outputId": "3093dedb-3c46-405e-a664-dd67357aea71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "trainX.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArvFmP9B5xlA",
        "colab_type": "code",
        "outputId": "6be641ad-143e-4fff-abab-9ad6ae396368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "trainY.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8_dl91S51D1",
        "colab_type": "code",
        "outputId": "03c7934b-d963-4aa0-8bbb-3d9e6edfecad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "testX.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQzRpzT858N1",
        "colab_type": "code",
        "outputId": "2f8d360a-9389-45c1-9973-945e64260d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "testY.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7yzeTj26BTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert train and test into categorical values using one-hot coding\n",
        "\n",
        "trainY = tf.keras.utils.to_categorical(trainY)\n",
        "testY = tf.keras.utils.to_categorical(testY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RHV3b9mzOIUq",
        "outputId": "3d7fff7e-6f89-4820-e76c-cc4aceab9ede",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# Shape of Y in the Train set following the encoding\n",
        "\n",
        "print(trainY.shape)\n",
        "print('First 5 examples now are: ', trainY[0:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n",
            "First 5 examples now are:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwhQ8e7VOIUw"
      },
      "source": [
        "### Visualize the data\n",
        "\n",
        "Plot first 10 images in the triaining set and their labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AvDML2OoOIUx",
        "outputId": "5eec52b7-b739-4fba-821f-1073fdcd43dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        }
      },
      "source": [
        "# First 10 images and their labels in the Training set\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(10):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(trainX[i], cmap=plt.cm.binary)\n",
        "    plt.show()\n",
        "\n",
        "print (\"Labels\", trainY[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHMAAABzCAYAAACrQz3mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAA7FJREFUaIHtmstKK0sUhr/qSxJCQiJEyMzMfABF\ncCpOdOTEqeDY5xCcCIIv4Vv4Aj6ACEI24iXYXgIx5NKd3oNmFZ24PUmOVeewY/8QUk0Ken291qpa\nqzoqjmN+gpz/24D/ShnooikDXTRloIumDHTR5M0zuVarxY1GY6a5o9GI19dXAFqtFkopfN8HQCmF\nUoooigCI45jBYEC1WgWgVCqxtLQ0032azSZBEKhp8+YCbTQaXF1dzTS30+lwcXEBwNnZGblcjuXl\nZQByuRy+79PpdADo9/vc3d2xt7cHwObmJvv7+zPdZ319faZ5c4HOo1KpRKVSAeDk5ITj42Our6+B\nxMP9fl97sFwus729ze7uLoB+ACZlDRQSTwFUq1WOjo44Pz8HIJ/Pj4Gura1xeHhIs9kE0J43Kaug\n5XIZgCAIWFlZ4fT0FID7+3uen5+RfK/VagRBQBiGQJKzppWtuibkuq4ev7y86HGtVqNer9PtdoHE\nw67rolSyeMq3SVkFlRBUSuG6rt5O3t/f/zhXACWETcoqaHr7KBQKGtRxHKIoGsvF0WjEaDQCoNfr\nGbcly1ETSq+icRxrj/3p2vM8fS2eNymroGJ4sVgkiqIxsPRCJbmZz+et2ZKFrgmJByEJR8dx9Dj9\nGyRhLh5ttVrGbbEG+vb2NtadKKU+wYkcxyEMQwqFAgDdbpder6evTcgaaD6f1x6aLADkWraXyUWo\nUqkYhYQsR7+vdGM963zpdmzImkfTkLIIjd3YcfRJg0iqI9d1v8znfytroL1eT4MopYjjGMdxxlZe\nKRzkk54/GAyM2pPl6HclXpJxOkS/aqwni3yTsgb6TwuRhLJIHoTUxlEUGW/VrIWu1LPSUEsRL55K\n56PjOJ/A2+22UXuyHP2uhsPhWLeS/v7qqMTzPP276ebbGmgYhhosiqKp50ACCeD7vvGTQGuhm94H\nJQ+nSXLW930+Pj6M2vNjctSqR8VDnudNDd30vun7Pre3t0btsQb68PCgxwIhJeBk/k2eJXmeZ/y1\nRBa631WhUGA4HDIcDnXLJh/XdceKeXlvKh7vdDoUi0Wj9ljbXjY2Nri5uQGSk/n0iYGE52TePj4+\nAgnw6uqqUXusgRaLRQ4ODgC4vLwkCAK9ZYRhqL0IyT7reZ5+u7a1tWXcoz8mR622aRKuOzs7APo/\nDU9PT7TbbR269Xqder3+KbxNvlVT85RaSqln4Jexu5vRShzHU/eiuUD/Zv2YHM1AF00Z6KIpA100\nZaCLpt9RO92fNHGEdgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABGZJREFUaIHtmjtLO2kUxn9zi/FCvEwEhaABRRSJ\nIFhppYWFWAp+Aq0EP4yFha3fQBsLRRDEQhEUCQpCtlCCBqJRcpvbFrPzbjS7/I3O/HeN81Q5zGHm\neeac854z7xvJcRx+AuT/msDvQii02RAKbTaEQpsNodBmg9qIczwed5LJ5Id8b25uME3TfYiqkkwm\niUQib3wMwwDg+voa27bF9ZGREWT5YzHIZDLkcjnpV34NCU0mk5yenn7Id25ujnw+D0A8Hmdra4v3\nL+n+/h6A2dlZSqUSAwMDAOzt7dHe3v6h50xNTX3IryGhjcC2bXK5HAB3d3ekUik6OjoAWFpaYnt7\nG8uyAIhGo3R1dfHy8gLwYZGNILAa7enpQZZlZFlG13Xa2trIZrNks1k2NjZIpVLiend3N6Zpous6\nuq4HwicwoUNDQxiGgWEYKIpCS0vLm+vJZJLn52een59RFAXTNCkWixSLxUD4hKvuVzE2NoZt2wBI\nkkR7e7tYVS8uLgC3NgEcx8EwDGKxWFB0ghOaSCRQVff2lmVRrVbp7+8HYHJyklgsRiKRABAvpLOz\nMyg6wQnt7+9H0zTAjZiiKMIeHx/HMIw3AiuVirCDQFijX0Vvby+ZTAaA0dFRotEo3v6UNxHVRlyW\n5brJyU8EJrSvr0/89hab2tSUJEmMiKZpYlkW3d3dQdEJU9cXeO3jPSRJQpIkFEVxSagqjuN8z/YC\nrqB/sh3HEWLBbS9erw0KgQp9vznu2aZpoqqqGOq9mffh4SEwLmGN+oF/i6iXqrUR1jRNtKMg8NuE\n2rb9xvbqFNwRUVGU7yn05uaGarUKuItQbQ91HKdOtKqq4kM9CPyYGg1MaDqdJpFIkEgkiEQibyJo\nWVadraqq2IE4Pj72nU9gQvf390WvtG27LnVre6y3OA0PDzM8PMzm5qbvfAKr0ZOTE/E9appmXQRr\nYds25XJZbLd8q4j+3xBYRDOZjPgaeZ+qlmXVjYe2bVMqlQDIZrNUKpW6DbWvwPeI5vN58vk8j4+P\naJqGpmlYloUsy6JWJUlClmXRZhzHoVKpMD8/z/z8PNFolLOzM195+R7R8/Pzv2/+V42WSiUhFKBc\nLlOtVsXXiyRJaJrG9fU14EY8nU4zPT3tG6+wRj+L3d1dwD1v8bZKFEVBlmVeX18Bt2ZN0xR2LBYT\nfdTzv7y89JWX70Jvb28BeHl5EcQty0LXdWHv7OywuLhIa2srAMViUZzLePbV1ZWvvMLU/SwWFxcB\nODw8FEOCLMtvzlS86HmLlZfinh2NRkmlUr7y8l3oysoKAKurq0KorutihQXEIW88Hgfg6emJSCRC\noVAAoFAosL6+7iuvwAaGi4sLJiYmhF3b/L0tE69my+UylmWJ89G9vT0GBwd95RPW6FeRSqVE6h4d\nHZFOpzk4OABgZmYGgLW1NcCN8PLyMgsLC0HRQWrkb6ySJD0CfwTG5nMYdByn91dODQn9zvgxNRoK\nbTaEQpsNodBmQyi02fAnDMDzwDW4zbQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6P8Zf4yPdoluNXSJbjV0iW41dIluNfwF+Cs2dD0MvyUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABCNJREFUaIHtmMtK+1oUh7+0idrU4qUtKNYLIiKo\nMy/gyNfwNRyKzgRBZ4L6BN4o4syB4lBEUHHgQFFQB15aC16qbUwacwah25b/EY2nnsNJ84PQpPu2\nvr3X2nslkmVZlIN8/7UB/5Y8ULfJA3WbPFC3yQN1m2QnlSORiNXW1vZLpvxMl5eXpFIp6at6jkDb\n2trY39//uVW/oL6+vm/VKxvX9UDdJg/UbfJA3SYPtBSyLEtcAKZpYpqmeM7lcuRyuT/avb+/f9qn\nYRj85DvXr4JKkiQu+ADPP8uyjCx/JGfz8/PMz88Ti8U+7VNRFNHeicrGdR3luj9VfhULVw9gaWkJ\ngKOjI+LxOFVVVQBEo1FGRkZYXl7+oy9d15menmZ8fNyRDb8GWuii+d+zszMA4vE4u7u7bG5uAtDe\n3k4sFiMUCgH2G8nGxsbf9ruyssLe3p5je/4R6Pv7Oz6f7f26rlNRUSHKCuPo8fGRsbExVldXAQgG\ngzQ2NjIwMADYG0wmk6GrqwuA6+trJiYmRPtkMinajo6OcnJywsHBgSNbvRj9SoXHBlC0mgDb29us\nra0BdizW19fT3d1tDyrLPD098fz8DEAgECAYDIp33YaGBhYXF5mZmRHlvb29ALy9vaFpmnDz7+rH\noJIk4ff7i/6bnZ1lYWEBgEQiQXNzMwA9PT3IskwikShqX3js+Hw+otEogJiAoaEhANbX10W7yclJ\n5ubmaG1tBWzw78gx6OHhIQBbW1ucnp6iaRoANzc3pNNpamtrAYjFYjw9PQlj8vcAqqqSy+WER/h8\nPgzDEPEeCASoqqoSm05jYyOvr68ANDU10dnZSSaTAeD+/v5bdpdNjEpO0qmWlharo6MDgGw2i2VZ\nIjYNw0BRlKL0Ln+vqip+v1+ke5IkoWmaKH97eyuKcU3T0HVdlPv9fpEWKopCZWWlcO/Hx0fu7u5K\n+3EsHA4zNTUFwM7ODsfHx1xdXQGQTqd5eHjAMAy7Y1kWxiWTSVKplHBN0zTRdV3UzQNVV1cD9vFT\nUVEh9gBJkkQyoes6qqpSWVkJIEKlpKBgbywAg4ODwMdmcHFxwfn5OZeXl4Ads/n4tSyr6MwNh8OE\nQiHC4bAwtqamRhitqiqqqooxC1cXIBKJEAwGAejv7/+W3WUTo45WtPA4ub29LZrl+vp6hoeHxSoq\niiLKTNNEkiThypqmCfcFO741TePl5QWwd9J0Oi1cW1EUEd+ZTIZQKCTy5mw2W3pQQLhM/jevbDaL\nYRhiMl5eXoRb5w02TROwU8fCSctPRD4JaGpqwrIsAVfYr2ma+P1+MX4gEPgd0M8UCASKBq2rqytV\n1yVR2cSoB+o2eaBukwfqNnmgbpOj91FJku6Bq98z50dqtSwr+lUlR6D/Z5WN63qgbpMH6jZ5oG6T\nB+o2/QXlqM5ZmVUhHAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABnpJREFUaIHtmU1rE1sYx38zmTOTNGmMNVqlWvMR\nFAURFy5cqYuiW/0ALkQ/QzdScNOdK7e6ELQoou4UF1UsguBOEOsLqVTTyqTpvJ+7iOfcTG+vySTo\n5bb5w8CcmTPnPP95Xs7znGNIKdkOMP9rAf4UhkS3GoZEtxqGRLcahkS3GqwsnavVqqzVaj31DYKA\n79+/A7C0tESSJF2/MQwDgP3797Nnz56e5vnw4QPfvn0zunaUUvZ8HTlyRP4b4jhOtT99+iSnp6fl\n9PS0LJfLEvjlZZqmdBxHOo4jZ2dnNx1/4xxSSvlTpq6yG1ly3aNHj8qFhYXUM6Up02x7weHDhwF4\n9+4dvu8DMDIywsjICJ7nAbBz504qlQr1eh2A9fV1CoWCft9sNhkbG+PUqVMA3Lp1KzWfmuunTCws\nLHTVaCbT3QgpZWrS48eP8/btWwDGx8cJggBom2QQBFhWe7qlpSXq9TqFQgEA27bxPI98Pg9APp8n\niiJu374NQKvVYm5uDmj/UCmlNvNeMRDRzsnu3bvHixcvOHDgAND+82EY6n7qAiiXy0gptTWoH6Y0\nahgGQggmJycBePLkCY8ePQLg9OnTmUnCMOp2RxzH5HI53T5//jzVahXXdQGoVCoIIQAIwxDDMIii\nCGhrrNPkgVRbmacy/UqlwpkzZwCo1+vs3btXj9Ur+taoIjk1NcXU1BSVSoVSqcTq6iqrq6taYNM0\nEUKQy+VS5rupMD/7G4ZBLpcjSRKSJCGfz+vr6dOnev7OH/3biCrMz88zPz8PoKOsElpBaVAJt1Gb\nnf3UpfzWNE3CMMTzPDzP49WrV6m+vWLb+OjARAuFAoVCASmlXj6gHUmFEAghMAwDy7K0Ri3LwnEc\n/d6yLCzLwrZtbNvWvq3MNUkSisUixWIxtaZmwUDLy5s3b1heXgZgx44deJ6HbdsAqXVRBS61nGz0\nV7UuquQlSRIsy9LtlZUVHMdpC2z1J/K2Md2BNBpFUSpZbzabOtBIKfUSoJIDpaE4jjFNM5UwdEZQ\n1Y7jGAAhhB738+fPfck6kEZfv35NEAQEQaBNT/lZoVDA931839cklLmqJURFVfUujmN9Afp+fX1d\n+2upVOLly5d/lmhndZDL5XQQ2fheCEEURZpYJyFFSpE1DAPf97FtW/+oKIp04PJ9n9nZ2T9L9P+E\ngXy0VCrp+ziOEULoqCulTGkL+GXxrcweYHV1VVc80E4B1beq0smKgYheu3ZNm6syq0ajAcCuXbvI\nUuvGcayJ5nI5fN/X1U+pVKLVagHt2nZubi7T2DAg0ffv3+v1zfd9giDg4MGDQLuGzCqM6i+EwHXd\nVJmnNBrHMbVaLXOptm18tG+iX758SYX9Vqul1zsVWdW9ynxUCpgkCXEc6+oE2uaqorTjODrLsm1b\n7zhEUYRhGHz8+DGzvH2b7vPnz7WA0N71E0LotK/RaGifUyQ7ze1Xpuc4DsViUa+xzWZTJx8q982K\nvomqIKR8VGU+qhaN41jnpWpDSwmYJAlCiJQPx3GsyasfpuZYWVnREXyY63ZB3xo9efIk8LcJmqaJ\nZVlaa0KIlLY7t1KiKKJQKOioqvJa9V4IoX0c2lsxWXYTNkPfRB8+fAig/dC2bZaXlxkfH9dtZZph\nGOodBmibuyIP6MCk2lEUpXyxc3+qX8J9E338+HF7gJ8+4zgOruty48YNAC5cuJBa8E3T1D9FReXO\ngNS53el5Hj9+/NBWs7i4SKVSSc3/9evXTPIOfbQbfN9ndHRUp2bKn86dOwfAlStX9LaH67o0Gg32\n7dunvwVS5ZsQgmazqZ8fO3aMq1evAvDs2TOtffXN/fv3M8nbN1HDMHBd9x8mpTAzM8PMzEzqmTJN\n13VTxwqGYWDbNuVy+V/n6/T3fD7PgwcPMsk7NN1uuHnzJnfv3mVtbQ345ynXZug8RMqCWq2mN+Eq\nlQqe53HixAkA7ty509sgg5yPLi4uyomJCTkxMSFHR0flxYsXNzs61eebYRjKMAyl7/syCAJ9qeed\nZ6BJkuhvL126JC3LkpZlyWq1Ks+ePZv5fHSgMm1yclIXx67rpjau1tbWKBaLut25P9QLOlPIQ4cO\n6ftms8nly5czyzr00V4gpeT69esAjI2N6eUD/k72+0VnMrF79259aOw4TibL0ONl2QUwDGMZWMw8\ny+/FQSnl7m6dMhH9P2Pb+OiQ6FbDkOhWw5DoVsOQ6FbDX5DfOmv/Lli9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Labels [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l4TbJGeSOIU4"
      },
      "source": [
        "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ac06XZZTOIU6",
        "colab": {}
      },
      "source": [
        "#Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "#Comile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3hQpLv3aOIU_"
      },
      "source": [
        "### Execute the model using model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O59C_-IgOIVB",
        "outputId": "be5383f6-73e2-4109-a35d-d891bf97be08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Executing the model with above parameters\n",
        "\n",
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=trainX.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 0s 7us/sample - loss: 202.9617 - acc: 0.1205 - val_loss: 6234.9214 - val_acc: 0.4079\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 6217.9370 - acc: 0.4103 - val_loss: 8814.1143 - val_acc: 0.2745\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 0s 5us/sample - loss: 8845.0176 - acc: 0.2717 - val_loss: 11826.9043 - val_acc: 0.2729\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 11798.7246 - acc: 0.2761 - val_loss: 13906.1650 - val_acc: 0.1398\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 13856.8584 - acc: 0.1390 - val_loss: 10329.0576 - val_acc: 0.1739\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 10214.8232 - acc: 0.1725 - val_loss: 12310.6660 - val_acc: 0.2184\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 12231.0039 - acc: 0.2253 - val_loss: 15320.2002 - val_acc: 0.3645\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 15239.3809 - acc: 0.3697 - val_loss: 13316.9883 - val_acc: 0.3051\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 13270.2314 - acc: 0.3095 - val_loss: 12393.3535 - val_acc: 0.3320\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 12385.0029 - acc: 0.3324 - val_loss: 9356.4092 - val_acc: 0.3731\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 9345.9072 - acc: 0.3683 - val_loss: 6366.1396 - val_acc: 0.4655\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 6271.9414 - acc: 0.4661 - val_loss: 3530.8008 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3460.4800 - acc: 0.5059 - val_loss: 4513.6685 - val_acc: 0.3739\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 4442.7207 - acc: 0.3785 - val_loss: 5345.6265 - val_acc: 0.5182\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 5279.8965 - acc: 0.5234 - val_loss: 6492.6138 - val_acc: 0.4437\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 6402.6631 - acc: 0.4507 - val_loss: 5425.6831 - val_acc: 0.5843\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 5346.8950 - acc: 0.5898 - val_loss: 3499.2217 - val_acc: 0.5526\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3465.0876 - acc: 0.5562 - val_loss: 2904.9180 - val_acc: 0.5670\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2853.0933 - acc: 0.5758 - val_loss: 4022.5740 - val_acc: 0.5828\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3964.3770 - acc: 0.5884 - val_loss: 4566.0210 - val_acc: 0.6259\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 4488.4961 - acc: 0.6317 - val_loss: 1979.2452 - val_acc: 0.6166\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1934.9249 - acc: 0.6230 - val_loss: 1431.9062 - val_acc: 0.5496\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1374.6873 - acc: 0.5583 - val_loss: 3018.8845 - val_acc: 0.6035\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2945.9771 - acc: 0.6085 - val_loss: 3481.2263 - val_acc: 0.5153\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3398.9712 - acc: 0.5186 - val_loss: 5219.7998 - val_acc: 0.6217\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 5168.5088 - acc: 0.6256 - val_loss: 4671.8242 - val_acc: 0.6543\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 4597.4541 - acc: 0.6597 - val_loss: 2328.6375 - val_acc: 0.6471\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2289.7515 - acc: 0.6463 - val_loss: 1532.8380 - val_acc: 0.6526\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1475.1006 - acc: 0.6554 - val_loss: 2046.6902 - val_acc: 0.5972\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1991.0381 - acc: 0.6021 - val_loss: 1443.6742 - val_acc: 0.6192\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1389.4225 - acc: 0.6196 - val_loss: 2263.4976 - val_acc: 0.6332\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2195.0913 - acc: 0.6435 - val_loss: 1816.8370 - val_acc: 0.6991\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1772.8257 - acc: 0.7060 - val_loss: 804.1311 - val_acc: 0.7199\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 768.5964 - acc: 0.7307 - val_loss: 1287.4683 - val_acc: 0.6505\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1216.7107 - acc: 0.6614 - val_loss: 2056.4492 - val_acc: 0.6964\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2019.4302 - acc: 0.7048 - val_loss: 1747.7908 - val_acc: 0.6636\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1701.3174 - acc: 0.6686 - val_loss: 2733.2075 - val_acc: 0.6464\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2633.7720 - acc: 0.6615 - val_loss: 2650.9397 - val_acc: 0.6710\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2594.0200 - acc: 0.6731 - val_loss: 2935.9336 - val_acc: 0.6498\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2885.5508 - acc: 0.6578 - val_loss: 2468.5400 - val_acc: 0.6780\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2387.7429 - acc: 0.6870 - val_loss: 2558.4792 - val_acc: 0.6748\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 0s 5us/sample - loss: 2497.0833 - acc: 0.6801 - val_loss: 2776.7175 - val_acc: 0.6599\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2730.5706 - acc: 0.6652 - val_loss: 2865.2827 - val_acc: 0.6345\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2758.1182 - acc: 0.6483 - val_loss: 3479.7837 - val_acc: 0.6722\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3412.8738 - acc: 0.6770 - val_loss: 3334.1379 - val_acc: 0.6654\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3286.7544 - acc: 0.6713 - val_loss: 1825.9534 - val_acc: 0.7014\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1768.5048 - acc: 0.7076 - val_loss: 1270.7184 - val_acc: 0.6291\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1191.7201 - acc: 0.6461 - val_loss: 2511.3381 - val_acc: 0.6988\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2470.8870 - acc: 0.7104 - val_loss: 1335.4152 - val_acc: 0.7095\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1292.5436 - acc: 0.7168 - val_loss: 1168.6426 - val_acc: 0.7040\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1111.7490 - acc: 0.7113 - val_loss: 1323.5618 - val_acc: 0.6891\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1270.0552 - acc: 0.6969 - val_loss: 2244.1211 - val_acc: 0.6403\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2178.4431 - acc: 0.6482 - val_loss: 1885.7360 - val_acc: 0.6620\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1840.8000 - acc: 0.6726 - val_loss: 1592.7618 - val_acc: 0.6760\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1540.0959 - acc: 0.6854 - val_loss: 2129.5569 - val_acc: 0.5528\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 0s 5us/sample - loss: 2015.9021 - acc: 0.5632 - val_loss: 3822.1621 - val_acc: 0.6782\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3776.5261 - acc: 0.6886 - val_loss: 3136.4600 - val_acc: 0.6711\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3074.6421 - acc: 0.6787 - val_loss: 2796.5083 - val_acc: 0.6833\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2750.0137 - acc: 0.6859 - val_loss: 1851.6196 - val_acc: 0.7128\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1781.9994 - acc: 0.7285 - val_loss: 1206.6062 - val_acc: 0.7385\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 0s 5us/sample - loss: 1174.7195 - acc: 0.7456 - val_loss: 833.7966 - val_acc: 0.7336\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 774.5266 - acc: 0.7478 - val_loss: 1322.8969 - val_acc: 0.7352\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1296.9550 - acc: 0.7455 - val_loss: 737.6029 - val_acc: 0.7331\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 682.2839 - acc: 0.7484 - val_loss: 1136.4127 - val_acc: 0.7242\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1111.5640 - acc: 0.7322 - val_loss: 1000.3371 - val_acc: 0.6724\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 926.1841 - acc: 0.6898 - val_loss: 2170.2119 - val_acc: 0.7181\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2145.3816 - acc: 0.7245 - val_loss: 1203.7261 - val_acc: 0.6899\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1161.0651 - acc: 0.6960 - val_loss: 1872.8988 - val_acc: 0.6678\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1818.4324 - acc: 0.6733 - val_loss: 1262.7068 - val_acc: 0.7294\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1216.8553 - acc: 0.7363 - val_loss: 853.7619 - val_acc: 0.7460\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 0s 5us/sample - loss: 808.1584 - acc: 0.7524 - val_loss: 1343.9980 - val_acc: 0.7310\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1288.6731 - acc: 0.7395 - val_loss: 1233.9966 - val_acc: 0.7189\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1193.8650 - acc: 0.7229 - val_loss: 2736.5098 - val_acc: 0.6330\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2625.8379 - acc: 0.6456 - val_loss: 3159.1367 - val_acc: 0.7007\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 0s 5us/sample - loss: 3105.1726 - acc: 0.7072 - val_loss: 3530.5168 - val_acc: 0.6631\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3474.1768 - acc: 0.6722 - val_loss: 2058.8977 - val_acc: 0.7105\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1999.8014 - acc: 0.7182 - val_loss: 1622.1725 - val_acc: 0.6188\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1525.8381 - acc: 0.6322 - val_loss: 2975.5972 - val_acc: 0.7139\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2933.3984 - acc: 0.7248 - val_loss: 2016.7592 - val_acc: 0.7045\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1961.5004 - acc: 0.7085 - val_loss: 2245.8760 - val_acc: 0.6817\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 0s 5us/sample - loss: 2200.1426 - acc: 0.6857 - val_loss: 2966.6528 - val_acc: 0.6034\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2838.3550 - acc: 0.6146 - val_loss: 3793.8379 - val_acc: 0.6579\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3729.8787 - acc: 0.6644 - val_loss: 3596.2524 - val_acc: 0.6396\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3534.8213 - acc: 0.6471 - val_loss: 1742.0576 - val_acc: 0.6764\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1681.4023 - acc: 0.6781 - val_loss: 1224.5988 - val_acc: 0.6840\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1146.0507 - acc: 0.7007 - val_loss: 1565.8002 - val_acc: 0.7256\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1533.5032 - acc: 0.7364 - val_loss: 694.9585 - val_acc: 0.7661\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 643.8117 - acc: 0.7789 - val_loss: 578.0881 - val_acc: 0.7696\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 541.3380 - acc: 0.7811 - val_loss: 734.3790 - val_acc: 0.7636\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 678.5219 - acc: 0.7763 - val_loss: 879.3308 - val_acc: 0.7504\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 831.7667 - acc: 0.7563 - val_loss: 1554.7133 - val_acc: 0.7055\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1497.2704 - acc: 0.7168 - val_loss: 1223.2155 - val_acc: 0.7541\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1172.0365 - acc: 0.7587 - val_loss: 893.2262 - val_acc: 0.7577\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 838.8344 - acc: 0.7689 - val_loss: 979.7960 - val_acc: 0.7435\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 933.5330 - acc: 0.7479 - val_loss: 1942.5916 - val_acc: 0.6884\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1879.3887 - acc: 0.7018 - val_loss: 1475.3666 - val_acc: 0.7387\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1429.2821 - acc: 0.7475 - val_loss: 749.5047 - val_acc: 0.7411\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 701.9676 - acc: 0.7528 - val_loss: 830.7294 - val_acc: 0.7490\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 790.0057 - acc: 0.7606 - val_loss: 1237.4174 - val_acc: 0.7035\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1163.6821 - acc: 0.7121 - val_loss: 2015.4064 - val_acc: 0.7372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3bb77cc5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZj3ENXDBTCN",
        "colab_type": "code",
        "outputId": "a6e4cdca-586d-446e-bb1b-4eb8d41f2194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "model.summary()\n",
        "\n",
        "# Accuracy is close to 74% with the above parameters"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JdzDtGwDOIVF"
      },
      "source": [
        "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WSuW7MDBRON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Repeat the model by adding Normalization layer\n",
        "\n",
        "#Initialize Sequential model\n",
        "model1 = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model1.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Normalize the data\n",
        "model1.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model1.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "#Comile the model\n",
        "model1.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kndfpdidOIVI",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mwk3T5LJOIVN"
      },
      "source": [
        "### Execute the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JNLR8tcBOIVP",
        "outputId": "b454b4ac-94e3-4cf1-f02b-6ead6de330e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Executing the model with above parameters\n",
        "\n",
        "model1.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=trainX.shape[0])\n",
        "\n",
        "# With batch Normalisation, accuracy for an epoch of 100 ended at about 72%"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 3.3318 - acc: 0.0696 - val_loss: 26.3452 - val_acc: 0.0544\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.9600 - acc: 0.0980 - val_loss: 15.4771 - val_acc: 0.0956\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.6563 - acc: 0.1341 - val_loss: 10.8217 - val_acc: 0.1409\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.4115 - acc: 0.1752 - val_loss: 8.3032 - val_acc: 0.1775\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.2144 - acc: 0.2195 - val_loss: 6.7631 - val_acc: 0.2043\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.0546 - acc: 0.2664 - val_loss: 5.7326 - val_acc: 0.2273\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9238 - acc: 0.3123 - val_loss: 4.9950 - val_acc: 0.2456\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8156 - acc: 0.3535 - val_loss: 4.4417 - val_acc: 0.2631\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7252 - acc: 0.3889 - val_loss: 4.0130 - val_acc: 0.2832\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6490 - acc: 0.4191 - val_loss: 3.6726 - val_acc: 0.3023\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.5841 - acc: 0.4433 - val_loss: 3.3965 - val_acc: 0.3235\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.5281 - acc: 0.4646 - val_loss: 3.1680 - val_acc: 0.3451\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.4792 - acc: 0.4821 - val_loss: 2.9755 - val_acc: 0.3639\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.4362 - acc: 0.4976 - val_loss: 2.8109 - val_acc: 0.3843\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.3978 - acc: 0.5117 - val_loss: 2.6683 - val_acc: 0.4032\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.3634 - acc: 0.5244 - val_loss: 2.5433 - val_acc: 0.4182\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.3323 - acc: 0.5367 - val_loss: 2.4327 - val_acc: 0.4331\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.3040 - acc: 0.5476 - val_loss: 2.3338 - val_acc: 0.4471\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.2781 - acc: 0.5574 - val_loss: 2.2448 - val_acc: 0.4592\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.2542 - acc: 0.5666 - val_loss: 2.1642 - val_acc: 0.4712\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.2322 - acc: 0.5743 - val_loss: 2.0908 - val_acc: 0.4836\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.2117 - acc: 0.5816 - val_loss: 2.0235 - val_acc: 0.4947\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.1927 - acc: 0.5884 - val_loss: 1.9617 - val_acc: 0.5054\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.1749 - acc: 0.5954 - val_loss: 1.9045 - val_acc: 0.5156\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.1582 - acc: 0.6019 - val_loss: 1.8516 - val_acc: 0.5258\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.1425 - acc: 0.6082 - val_loss: 1.8023 - val_acc: 0.5337\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.1277 - acc: 0.6140 - val_loss: 1.7564 - val_acc: 0.5399\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.1138 - acc: 0.6197 - val_loss: 1.7135 - val_acc: 0.5486\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.1006 - acc: 0.6249 - val_loss: 1.6733 - val_acc: 0.5561\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.0881 - acc: 0.6299 - val_loss: 1.6355 - val_acc: 0.5619\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.0762 - acc: 0.6349 - val_loss: 1.6000 - val_acc: 0.5667\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.0649 - acc: 0.6399 - val_loss: 1.5666 - val_acc: 0.5720\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.0542 - acc: 0.6441 - val_loss: 1.5350 - val_acc: 0.5782\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.0439 - acc: 0.6470 - val_loss: 1.5052 - val_acc: 0.5824\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.0341 - acc: 0.6505 - val_loss: 1.4769 - val_acc: 0.5870\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.0247 - acc: 0.6542 - val_loss: 1.4501 - val_acc: 0.5918\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.0157 - acc: 0.6572 - val_loss: 1.4247 - val_acc: 0.5958\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.0071 - acc: 0.6608 - val_loss: 1.4005 - val_acc: 0.6004\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9988 - acc: 0.6642 - val_loss: 1.3775 - val_acc: 0.6033\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9908 - acc: 0.6670 - val_loss: 1.3556 - val_acc: 0.6073\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9831 - acc: 0.6697 - val_loss: 1.3348 - val_acc: 0.6100\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9757 - acc: 0.6726 - val_loss: 1.3148 - val_acc: 0.6139\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9686 - acc: 0.6746 - val_loss: 1.2958 - val_acc: 0.6166\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9617 - acc: 0.6770 - val_loss: 1.2775 - val_acc: 0.6194\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9551 - acc: 0.6795 - val_loss: 1.2601 - val_acc: 0.6227\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9487 - acc: 0.6818 - val_loss: 1.2434 - val_acc: 0.6255\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9424 - acc: 0.6840 - val_loss: 1.2273 - val_acc: 0.6293\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9364 - acc: 0.6857 - val_loss: 1.2120 - val_acc: 0.6327\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9306 - acc: 0.6875 - val_loss: 1.1972 - val_acc: 0.6359\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9249 - acc: 0.6892 - val_loss: 1.1830 - val_acc: 0.6388\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9195 - acc: 0.6910 - val_loss: 1.1693 - val_acc: 0.6407\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9141 - acc: 0.6929 - val_loss: 1.1562 - val_acc: 0.6426\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9090 - acc: 0.6948 - val_loss: 1.1435 - val_acc: 0.6448\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.9040 - acc: 0.6963 - val_loss: 1.1313 - val_acc: 0.6470\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8991 - acc: 0.6981 - val_loss: 1.1196 - val_acc: 0.6489\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8943 - acc: 0.6994 - val_loss: 1.1082 - val_acc: 0.6512\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8897 - acc: 0.7011 - val_loss: 1.0973 - val_acc: 0.6529\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8852 - acc: 0.7029 - val_loss: 1.0867 - val_acc: 0.6544\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8808 - acc: 0.7043 - val_loss: 1.0765 - val_acc: 0.6565\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8765 - acc: 0.7057 - val_loss: 1.0666 - val_acc: 0.6589\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8723 - acc: 0.7069 - val_loss: 1.0570 - val_acc: 0.6615\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8683 - acc: 0.7080 - val_loss: 1.0477 - val_acc: 0.6642\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8643 - acc: 0.7095 - val_loss: 1.0388 - val_acc: 0.6672\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8604 - acc: 0.7111 - val_loss: 1.0301 - val_acc: 0.6688\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8566 - acc: 0.7122 - val_loss: 1.0217 - val_acc: 0.6702\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8529 - acc: 0.7135 - val_loss: 1.0135 - val_acc: 0.6716\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8493 - acc: 0.7146 - val_loss: 1.0056 - val_acc: 0.6729\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8457 - acc: 0.7160 - val_loss: 0.9979 - val_acc: 0.6748\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8423 - acc: 0.7173 - val_loss: 0.9905 - val_acc: 0.6767\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8389 - acc: 0.7183 - val_loss: 0.9832 - val_acc: 0.6786\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8356 - acc: 0.7195 - val_loss: 0.9762 - val_acc: 0.6804\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8323 - acc: 0.7208 - val_loss: 0.9694 - val_acc: 0.6812\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8291 - acc: 0.7217 - val_loss: 0.9627 - val_acc: 0.6832\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8260 - acc: 0.7226 - val_loss: 0.9563 - val_acc: 0.6847\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8229 - acc: 0.7238 - val_loss: 0.9500 - val_acc: 0.6866\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8199 - acc: 0.7248 - val_loss: 0.9439 - val_acc: 0.6882\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8170 - acc: 0.7257 - val_loss: 0.9379 - val_acc: 0.6892\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8141 - acc: 0.7266 - val_loss: 0.9321 - val_acc: 0.6914\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8113 - acc: 0.7278 - val_loss: 0.9265 - val_acc: 0.6927\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8085 - acc: 0.7286 - val_loss: 0.9210 - val_acc: 0.6936\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8058 - acc: 0.7293 - val_loss: 0.9157 - val_acc: 0.6945\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8031 - acc: 0.7303 - val_loss: 0.9104 - val_acc: 0.6955\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8005 - acc: 0.7313 - val_loss: 0.9054 - val_acc: 0.6974\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7979 - acc: 0.7322 - val_loss: 0.9004 - val_acc: 0.6983\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7954 - acc: 0.7329 - val_loss: 0.8956 - val_acc: 0.7000\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7929 - acc: 0.7337 - val_loss: 0.8908 - val_acc: 0.7017\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7904 - acc: 0.7345 - val_loss: 0.8862 - val_acc: 0.7030\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7880 - acc: 0.7355 - val_loss: 0.8817 - val_acc: 0.7039\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7856 - acc: 0.7364 - val_loss: 0.8774 - val_acc: 0.7053\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7833 - acc: 0.7370 - val_loss: 0.8731 - val_acc: 0.7062\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7810 - acc: 0.7378 - val_loss: 0.8689 - val_acc: 0.7072\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7788 - acc: 0.7385 - val_loss: 0.8648 - val_acc: 0.7085\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7765 - acc: 0.7391 - val_loss: 0.8608 - val_acc: 0.7093\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7744 - acc: 0.7397 - val_loss: 0.8569 - val_acc: 0.7100\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7722 - acc: 0.7402 - val_loss: 0.8531 - val_acc: 0.7111\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7701 - acc: 0.7411 - val_loss: 0.8494 - val_acc: 0.7117\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7680 - acc: 0.7417 - val_loss: 0.8457 - val_acc: 0.7123\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7660 - acc: 0.7422 - val_loss: 0.8421 - val_acc: 0.7135\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7639 - acc: 0.7428 - val_loss: 0.8387 - val_acc: 0.7148\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7619 - acc: 0.7436 - val_loss: 0.8352 - val_acc: 0.7153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3bb6b162b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Py-KwkmjOIVU"
      },
      "source": [
        "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLXUE9jWOIVV",
        "colab": {}
      },
      "source": [
        "# Customize the learning rate to 0.001 in SGD\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Repeat the model by adding Normalization layer\n",
        "\n",
        "#Initialize Sequential model\n",
        "model2 = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model2.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Normalize the data\n",
        "model2.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model2.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "#Comile the model\n",
        "sgd = SGD(lr=0.001)\n",
        "model2.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pJUqA5T4OIVc",
        "outputId": "0fa8462c-d3d7-45c3-eeed-046f852334cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Executing the mode with learning rate 0.001 of SGD\n",
        "\n",
        "model2.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=trainX.shape[0])\n",
        "\n",
        "# With the above parameters, accuracy for an epoch of 100 ended at about 41%"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 1s 16us/sample - loss: 3.2067 - acc: 0.0853 - val_loss: 29.3034 - val_acc: 0.1051\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 3.1732 - acc: 0.0879 - val_loss: 20.4714 - val_acc: 0.1083\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 3.1404 - acc: 0.0905 - val_loss: 16.4440 - val_acc: 0.1112\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 3.1082 - acc: 0.0933 - val_loss: 13.9991 - val_acc: 0.1148\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 3.0766 - acc: 0.0959 - val_loss: 12.3084 - val_acc: 0.1183\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 3.0455 - acc: 0.0991 - val_loss: 11.0479 - val_acc: 0.1224\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 3.0150 - acc: 0.1022 - val_loss: 10.0609 - val_acc: 0.1259\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.9851 - acc: 0.1055 - val_loss: 9.2611 - val_acc: 0.1279\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.9558 - acc: 0.1084 - val_loss: 8.5962 - val_acc: 0.1304\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.9270 - acc: 0.1118 - val_loss: 8.0325 - val_acc: 0.1323\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.8987 - acc: 0.1156 - val_loss: 7.5472 - val_acc: 0.1352\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.8709 - acc: 0.1189 - val_loss: 7.1241 - val_acc: 0.1384\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.8437 - acc: 0.1225 - val_loss: 6.7512 - val_acc: 0.1406\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.8169 - acc: 0.1260 - val_loss: 6.4198 - val_acc: 0.1434\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.7907 - acc: 0.1296 - val_loss: 6.1230 - val_acc: 0.1470\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.7649 - acc: 0.1334 - val_loss: 5.8554 - val_acc: 0.1498\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.7396 - acc: 0.1382 - val_loss: 5.6128 - val_acc: 0.1537\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.7148 - acc: 0.1422 - val_loss: 5.3918 - val_acc: 0.1572\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.6904 - acc: 0.1468 - val_loss: 5.1895 - val_acc: 0.1592\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.6665 - acc: 0.1509 - val_loss: 5.0036 - val_acc: 0.1612\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.6430 - acc: 0.1556 - val_loss: 4.8322 - val_acc: 0.1650\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.6200 - acc: 0.1602 - val_loss: 4.6736 - val_acc: 0.1686\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.5973 - acc: 0.1647 - val_loss: 4.5264 - val_acc: 0.1707\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.5751 - acc: 0.1697 - val_loss: 4.3894 - val_acc: 0.1721\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.5533 - acc: 0.1743 - val_loss: 4.2617 - val_acc: 0.1734\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.5318 - acc: 0.1789 - val_loss: 4.1422 - val_acc: 0.1768\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.5108 - acc: 0.1836 - val_loss: 4.0303 - val_acc: 0.1796\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.4901 - acc: 0.1884 - val_loss: 3.9252 - val_acc: 0.1830\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.4698 - acc: 0.1928 - val_loss: 3.8264 - val_acc: 0.1851\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.4499 - acc: 0.1975 - val_loss: 3.7333 - val_acc: 0.1879\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.4303 - acc: 0.2026 - val_loss: 3.6453 - val_acc: 0.1911\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.4110 - acc: 0.2072 - val_loss: 3.5622 - val_acc: 0.1947\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.3921 - acc: 0.2119 - val_loss: 3.4836 - val_acc: 0.1986\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.3735 - acc: 0.2166 - val_loss: 3.4090 - val_acc: 0.2010\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.3553 - acc: 0.2210 - val_loss: 3.3382 - val_acc: 0.2036\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.3374 - acc: 0.2252 - val_loss: 3.2710 - val_acc: 0.2068\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.3197 - acc: 0.2294 - val_loss: 3.2070 - val_acc: 0.2094\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.3024 - acc: 0.2334 - val_loss: 3.1460 - val_acc: 0.2139\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.2854 - acc: 0.2374 - val_loss: 3.0879 - val_acc: 0.2177\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.2687 - acc: 0.2418 - val_loss: 3.0324 - val_acc: 0.2222\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.2522 - acc: 0.2460 - val_loss: 2.9793 - val_acc: 0.2246\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.2360 - acc: 0.2499 - val_loss: 2.9286 - val_acc: 0.2282\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.2201 - acc: 0.2536 - val_loss: 2.8801 - val_acc: 0.2311\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.2045 - acc: 0.2576 - val_loss: 2.8336 - val_acc: 0.2349\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.1891 - acc: 0.2616 - val_loss: 2.7890 - val_acc: 0.2379\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.1740 - acc: 0.2650 - val_loss: 2.7462 - val_acc: 0.2413\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.1591 - acc: 0.2692 - val_loss: 2.7051 - val_acc: 0.2445\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.1445 - acc: 0.2731 - val_loss: 2.6656 - val_acc: 0.2485\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.1301 - acc: 0.2772 - val_loss: 2.6276 - val_acc: 0.2513\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.1160 - acc: 0.2810 - val_loss: 2.5910 - val_acc: 0.2555\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.1021 - acc: 0.2847 - val_loss: 2.5558 - val_acc: 0.2597\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.0884 - acc: 0.2883 - val_loss: 2.5218 - val_acc: 0.2624\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.0749 - acc: 0.2915 - val_loss: 2.4891 - val_acc: 0.2659\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.0616 - acc: 0.2954 - val_loss: 2.4575 - val_acc: 0.2688\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.0486 - acc: 0.2987 - val_loss: 2.4270 - val_acc: 0.2716\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.0357 - acc: 0.3019 - val_loss: 2.3976 - val_acc: 0.2750\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.0231 - acc: 0.3047 - val_loss: 2.3691 - val_acc: 0.2784\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.0106 - acc: 0.3081 - val_loss: 2.3415 - val_acc: 0.2818\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9984 - acc: 0.3108 - val_loss: 2.3149 - val_acc: 0.2840\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9863 - acc: 0.3136 - val_loss: 2.2891 - val_acc: 0.2867\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9744 - acc: 0.3162 - val_loss: 2.2641 - val_acc: 0.2903\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9627 - acc: 0.3191 - val_loss: 2.2399 - val_acc: 0.2930\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9512 - acc: 0.3223 - val_loss: 2.2164 - val_acc: 0.2961\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9399 - acc: 0.3253 - val_loss: 2.1937 - val_acc: 0.2983\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9287 - acc: 0.3282 - val_loss: 2.1716 - val_acc: 0.3015\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9177 - acc: 0.3312 - val_loss: 2.1501 - val_acc: 0.3052\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9069 - acc: 0.3344 - val_loss: 2.1293 - val_acc: 0.3086\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8962 - acc: 0.3368 - val_loss: 2.1091 - val_acc: 0.3132\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8857 - acc: 0.3391 - val_loss: 2.0894 - val_acc: 0.3163\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8753 - acc: 0.3417 - val_loss: 2.0703 - val_acc: 0.3203\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8651 - acc: 0.3443 - val_loss: 2.0517 - val_acc: 0.3239\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8551 - acc: 0.3469 - val_loss: 2.0337 - val_acc: 0.3276\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8452 - acc: 0.3496 - val_loss: 2.0161 - val_acc: 0.3313\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8354 - acc: 0.3519 - val_loss: 1.9989 - val_acc: 0.3339\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8258 - acc: 0.3548 - val_loss: 1.9823 - val_acc: 0.3375\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8163 - acc: 0.3573 - val_loss: 1.9660 - val_acc: 0.3406\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8069 - acc: 0.3601 - val_loss: 1.9502 - val_acc: 0.3439\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7977 - acc: 0.3627 - val_loss: 1.9347 - val_acc: 0.3467\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7887 - acc: 0.3652 - val_loss: 1.9197 - val_acc: 0.3504\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7797 - acc: 0.3679 - val_loss: 1.9050 - val_acc: 0.3524\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7709 - acc: 0.3703 - val_loss: 1.8907 - val_acc: 0.3559\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7622 - acc: 0.3730 - val_loss: 1.8767 - val_acc: 0.3590\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7536 - acc: 0.3757 - val_loss: 1.8631 - val_acc: 0.3618\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7452 - acc: 0.3786 - val_loss: 1.8498 - val_acc: 0.3651\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7369 - acc: 0.3813 - val_loss: 1.8368 - val_acc: 0.3673\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7286 - acc: 0.3840 - val_loss: 1.8241 - val_acc: 0.3698\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7205 - acc: 0.3865 - val_loss: 1.8116 - val_acc: 0.3731\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7126 - acc: 0.3887 - val_loss: 1.7995 - val_acc: 0.3762\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7047 - acc: 0.3912 - val_loss: 1.7877 - val_acc: 0.3800\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6969 - acc: 0.3936 - val_loss: 1.7761 - val_acc: 0.3837\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6893 - acc: 0.3959 - val_loss: 1.7648 - val_acc: 0.3861\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6817 - acc: 0.3982 - val_loss: 1.7537 - val_acc: 0.3894\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6743 - acc: 0.4010 - val_loss: 1.7428 - val_acc: 0.3923\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6669 - acc: 0.4031 - val_loss: 1.7322 - val_acc: 0.3955\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6597 - acc: 0.4057 - val_loss: 1.7219 - val_acc: 0.3980\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6526 - acc: 0.4085 - val_loss: 1.7117 - val_acc: 0.4016\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6455 - acc: 0.4108 - val_loss: 1.7018 - val_acc: 0.4036\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6386 - acc: 0.4137 - val_loss: 1.6920 - val_acc: 0.4052\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6317 - acc: 0.4162 - val_loss: 1.6825 - val_acc: 0.4089\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6250 - acc: 0.4185 - val_loss: 1.6732 - val_acc: 0.4117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3bb649c400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j9CSqKvpOIVk"
      },
      "source": [
        "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GGAad54JOIVm",
        "colab": {}
      },
      "source": [
        "# Model Building\n",
        "#Initialize Sequential Graph (model)\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Dense layer for prediction - Keras declares weights and bias automatically with 100 neurons\n",
        "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
        "\n",
        "#Add 2nd Dense layer with 100 neurons\n",
        "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
        "\n",
        "#Add 3rd Dense layer with 10 neurons\n",
        "model.add(tf.keras.layers.Dense(10, activation='sigmoid'))\n",
        "\n",
        "#Output layer with 10 neurons as there're 10 class values\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Model compilation with SGD and Cross-Entropy\n",
        "sgd = SGD(lr=0.03)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MQ7oIymROIVp",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X-O-fFxnOIVt",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BiP7IL52OIVw",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nr2YsZV0OIV0"
      },
      "source": [
        "## Review model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h4ojW6-oOIV2",
        "outputId": "6504d7fc-3d08-4672-a4a6-3e76e0c82643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Executing the mode with learning rate 0.001 of SGD\n",
        "\n",
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=trainX.shape[0])\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 2s 27us/sample - loss: 2.3887 - acc: 0.0948 - val_loss: 2.3792 - val_acc: 0.0834\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3856 - acc: 0.0918 - val_loss: 2.3765 - val_acc: 0.0806\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3826 - acc: 0.0873 - val_loss: 2.3739 - val_acc: 0.0777\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3797 - acc: 0.0820 - val_loss: 2.3714 - val_acc: 0.0745\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3769 - acc: 0.0779 - val_loss: 2.3689 - val_acc: 0.0712\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3743 - acc: 0.0747 - val_loss: 2.3665 - val_acc: 0.0686\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3717 - acc: 0.0763 - val_loss: 2.3642 - val_acc: 0.0653\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3692 - acc: 0.0799 - val_loss: 2.3620 - val_acc: 0.0628\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3668 - acc: 0.0866 - val_loss: 2.3599 - val_acc: 0.0599\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3645 - acc: 0.0930 - val_loss: 2.3578 - val_acc: 0.0577\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3623 - acc: 0.0966 - val_loss: 2.3558 - val_acc: 0.0566\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3601 - acc: 0.0996 - val_loss: 2.3539 - val_acc: 0.0568\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3581 - acc: 0.1005 - val_loss: 2.3520 - val_acc: 0.0581\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3561 - acc: 0.1014 - val_loss: 2.3502 - val_acc: 0.0597\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3542 - acc: 0.1020 - val_loss: 2.3485 - val_acc: 0.0636\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3523 - acc: 0.1029 - val_loss: 2.3468 - val_acc: 0.0655\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3505 - acc: 0.1029 - val_loss: 2.3452 - val_acc: 0.0691\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3488 - acc: 0.1023 - val_loss: 2.3436 - val_acc: 0.0730\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3471 - acc: 0.1022 - val_loss: 2.3421 - val_acc: 0.0749\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3455 - acc: 0.1022 - val_loss: 2.3406 - val_acc: 0.0765\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3440 - acc: 0.1022 - val_loss: 2.3392 - val_acc: 0.0784\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3425 - acc: 0.1016 - val_loss: 2.3379 - val_acc: 0.0799\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3410 - acc: 0.1015 - val_loss: 2.3365 - val_acc: 0.0813\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3396 - acc: 0.1016 - val_loss: 2.3353 - val_acc: 0.0831\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3383 - acc: 0.1017 - val_loss: 2.3340 - val_acc: 0.0848\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3370 - acc: 0.1018 - val_loss: 2.3328 - val_acc: 0.0866\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3357 - acc: 0.1019 - val_loss: 2.3317 - val_acc: 0.0880\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3345 - acc: 0.1019 - val_loss: 2.3306 - val_acc: 0.0895\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3333 - acc: 0.1018 - val_loss: 2.3295 - val_acc: 0.0903\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3322 - acc: 0.1017 - val_loss: 2.3284 - val_acc: 0.0917\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3311 - acc: 0.1015 - val_loss: 2.3274 - val_acc: 0.0927\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3300 - acc: 0.1015 - val_loss: 2.3264 - val_acc: 0.0944\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3289 - acc: 0.1016 - val_loss: 2.3255 - val_acc: 0.0956\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3279 - acc: 0.1017 - val_loss: 2.3246 - val_acc: 0.0971\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3270 - acc: 0.1017 - val_loss: 2.3237 - val_acc: 0.0984\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3260 - acc: 0.1019 - val_loss: 2.3228 - val_acc: 0.0998\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3251 - acc: 0.1019 - val_loss: 2.3219 - val_acc: 0.1009\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3242 - acc: 0.1019 - val_loss: 2.3211 - val_acc: 0.1021\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3234 - acc: 0.1021 - val_loss: 2.3203 - val_acc: 0.1041\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3226 - acc: 0.1023 - val_loss: 2.3196 - val_acc: 0.1058\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3218 - acc: 0.1026 - val_loss: 2.3188 - val_acc: 0.1072\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3210 - acc: 0.1032 - val_loss: 2.3181 - val_acc: 0.1085\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3202 - acc: 0.1037 - val_loss: 2.3174 - val_acc: 0.1109\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3195 - acc: 0.1041 - val_loss: 2.3167 - val_acc: 0.1137\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3188 - acc: 0.1049 - val_loss: 2.3160 - val_acc: 0.1152\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3181 - acc: 0.1056 - val_loss: 2.3154 - val_acc: 0.1179\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3174 - acc: 0.1062 - val_loss: 2.3147 - val_acc: 0.1197\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3167 - acc: 0.1069 - val_loss: 2.3141 - val_acc: 0.1222\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3161 - acc: 0.1077 - val_loss: 2.3135 - val_acc: 0.1237\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3155 - acc: 0.1085 - val_loss: 2.3129 - val_acc: 0.1268\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3149 - acc: 0.1094 - val_loss: 2.3124 - val_acc: 0.1291\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3143 - acc: 0.1102 - val_loss: 2.3118 - val_acc: 0.1311\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3137 - acc: 0.1112 - val_loss: 2.3113 - val_acc: 0.1340\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3131 - acc: 0.1123 - val_loss: 2.3108 - val_acc: 0.1360\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3126 - acc: 0.1134 - val_loss: 2.3102 - val_acc: 0.1380\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3121 - acc: 0.1143 - val_loss: 2.3097 - val_acc: 0.1407\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3115 - acc: 0.1154 - val_loss: 2.3093 - val_acc: 0.1419\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3110 - acc: 0.1167 - val_loss: 2.3088 - val_acc: 0.1437\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3105 - acc: 0.1177 - val_loss: 2.3083 - val_acc: 0.1462\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3101 - acc: 0.1190 - val_loss: 2.3079 - val_acc: 0.1487\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3096 - acc: 0.1200 - val_loss: 2.3074 - val_acc: 0.1502\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3091 - acc: 0.1211 - val_loss: 2.3070 - val_acc: 0.1518\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3087 - acc: 0.1223 - val_loss: 2.3066 - val_acc: 0.1525\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3082 - acc: 0.1235 - val_loss: 2.3061 - val_acc: 0.1544\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3078 - acc: 0.1247 - val_loss: 2.3057 - val_acc: 0.1563\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3074 - acc: 0.1261 - val_loss: 2.3053 - val_acc: 0.1580\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3070 - acc: 0.1275 - val_loss: 2.3049 - val_acc: 0.1589\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3066 - acc: 0.1285 - val_loss: 2.3046 - val_acc: 0.1606\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3062 - acc: 0.1298 - val_loss: 2.3042 - val_acc: 0.1622\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3058 - acc: 0.1308 - val_loss: 2.3038 - val_acc: 0.1635\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3054 - acc: 0.1321 - val_loss: 2.3035 - val_acc: 0.1643\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3050 - acc: 0.1333 - val_loss: 2.3031 - val_acc: 0.1654\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3047 - acc: 0.1346 - val_loss: 2.3028 - val_acc: 0.1666\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3043 - acc: 0.1359 - val_loss: 2.3024 - val_acc: 0.1682\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3040 - acc: 0.1372 - val_loss: 2.3021 - val_acc: 0.1691\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3036 - acc: 0.1387 - val_loss: 2.3018 - val_acc: 0.1698\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3033 - acc: 0.1400 - val_loss: 2.3015 - val_acc: 0.1714\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3030 - acc: 0.1415 - val_loss: 2.3012 - val_acc: 0.1722\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3027 - acc: 0.1426 - val_loss: 2.3008 - val_acc: 0.1729\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3023 - acc: 0.1441 - val_loss: 2.3005 - val_acc: 0.1738\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3020 - acc: 0.1451 - val_loss: 2.3002 - val_acc: 0.1754\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3017 - acc: 0.1462 - val_loss: 2.3000 - val_acc: 0.1763\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3014 - acc: 0.1475 - val_loss: 2.2997 - val_acc: 0.1766\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3011 - acc: 0.1486 - val_loss: 2.2994 - val_acc: 0.1776\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3008 - acc: 0.1499 - val_loss: 2.2991 - val_acc: 0.1782\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3006 - acc: 0.1511 - val_loss: 2.2988 - val_acc: 0.1792\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3003 - acc: 0.1523 - val_loss: 2.2986 - val_acc: 0.1797\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3000 - acc: 0.1534 - val_loss: 2.2983 - val_acc: 0.1809\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.2997 - acc: 0.1546 - val_loss: 2.2980 - val_acc: 0.1818\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.2995 - acc: 0.1556 - val_loss: 2.2978 - val_acc: 0.1825\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.2992 - acc: 0.1566 - val_loss: 2.2975 - val_acc: 0.1833\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.2989 - acc: 0.1577 - val_loss: 2.2973 - val_acc: 0.1843\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.2987 - acc: 0.1590 - val_loss: 2.2970 - val_acc: 0.1847\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.2984 - acc: 0.1599 - val_loss: 2.2968 - val_acc: 0.1856\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.2982 - acc: 0.1607 - val_loss: 2.2966 - val_acc: 0.1859\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.2979 - acc: 0.1615 - val_loss: 2.2963 - val_acc: 0.1864\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.2977 - acc: 0.1625 - val_loss: 2.2961 - val_acc: 0.1870\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.2974 - acc: 0.1633 - val_loss: 2.2959 - val_acc: 0.1877\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.2972 - acc: 0.1642 - val_loss: 2.2956 - val_acc: 0.1882\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.2970 - acc: 0.1651 - val_loss: 2.2954 - val_acc: 0.1882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3bb6abaf60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gfFGmbZLOIV5"
      },
      "source": [
        "### Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIkbMEN5OIV7",
        "outputId": "44c2bb6f-52ac-457a-8825-994b2781a070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Accuracy of the model\n",
        "\n",
        "model.evaluate(testX,testY)\n",
        "\n",
        "# 19% - Accuracy"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 35us/sample - loss: 2.2954 - acc: 0.1882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.2954134101867676, 0.1882]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    }
  ]
}