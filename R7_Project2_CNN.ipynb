{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kIWaR5ZpKlJ"
   },
   "source": [
    "## Dog Breed Classification\n",
    "\n",
    "In this project we will use traditional CNN, CNN with data augmentation and finally transfer Learning by VGG16 model with weights pre-trained on Imagenet to solve the dog breed classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7MDmaAw2xGO"
   },
   "source": [
    "### Load Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKSgCs_da7YK"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1q2zzIaUprk_"
   },
   "source": [
    "Now, upload the given dataset file shared with you in your google drive and give its path for the below given `project_path` variable. For example, a path is given below according to the file path in our google drive. You need to change this to match the path of yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tp6FvAToxUFs"
   },
   "outputs": [],
   "source": [
    "project_path = \"/content/drive/My Drive/DogBreed_Classification/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rydR_j8lqUei"
   },
   "source": [
    "Run the below code to extract all the images in the train.zip files given in the dataset. We are going to use these images as train and validation sets and their labels in further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3350WZM4w4EL"
   },
   "outputs": [],
   "source": [
    "# Extracting the Training set\n",
    "\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(project_path+'train.zip', 'r') as z:\n",
    "  z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NHq1iBCfFjE"
   },
   "source": [
    "Repeat the same step for test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fxzynvB2YCb"
   },
   "outputs": [],
   "source": [
    "# Extracting the Test set\n",
    "\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(project_path+'test.zip', 'r') as z:\n",
    "  z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jnUMhQrDfJmz"
   },
   "source": [
    "Repeat the same step for sample_submission.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PyTxE8q2jLf"
   },
   "outputs": [],
   "source": [
    "# Extracting the Sample Submission set\n",
    "\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(project_path+'sample_submission.csv.zip', 'r') as z:\n",
    "  z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2G9RIxB-fOLT"
   },
   "source": [
    "Repeat the same step for labels.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXtnEoEixbgi"
   },
   "outputs": [],
   "source": [
    "# Extracting the Sample Submission set\n",
    "\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(project_path+'labels.csv.zip', 'r') as z:\n",
    "  z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJc1lVrW_jmL"
   },
   "source": [
    "After this process, we will have 4 files - Train folder, test folder and labels.csv and sample_submission.csv as part of your google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYmJKmDqqpng"
   },
   "source": [
    "### Read labels.csv file using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "WmlJ2VMY96IZ",
    "outputId": "2d2418ca-7736-4e3d-97eb-25baeec6abe1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10217</th>\n",
       "      <td>ffd25009d635cfd16e793503ac5edef0</td>\n",
       "      <td>borzoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10218</th>\n",
       "      <td>ffd3f636f7f379c51ba3648a9ff8254f</td>\n",
       "      <td>dandie_dinmont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10219</th>\n",
       "      <td>ffe2ca6c940cddfee68fa3cc6c63213f</td>\n",
       "      <td>airedale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10220</th>\n",
       "      <td>ffe5f6d8e2bff356e9482a80a6e29aac</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10221</th>\n",
       "      <td>fff43b07992508bc822f33d8ffd902ae</td>\n",
       "      <td>chesapeake_bay_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10222 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                     breed\n",
       "0      000bec180eb18c7604dcecc8fe0dba07               boston_bull\n",
       "1      001513dfcb2ffafc82cccf4d8bbaba97                     dingo\n",
       "2      001cdf01b096e06d78e9e5112d419397                  pekinese\n",
       "3      00214f311d5d2247d5dfe4fe24b2303d                  bluetick\n",
       "4      0021f9ceb3235effd7fcde7f7538ed62          golden_retriever\n",
       "...                                 ...                       ...\n",
       "10217  ffd25009d635cfd16e793503ac5edef0                    borzoi\n",
       "10218  ffd3f636f7f379c51ba3648a9ff8254f            dandie_dinmont\n",
       "10219  ffe2ca6c940cddfee68fa3cc6c63213f                  airedale\n",
       "10220  ffe5f6d8e2bff356e9482a80a6e29aac        miniature_pinscher\n",
       "10221  fff43b07992508bc822f33d8ffd902ae  chesapeake_bay_retriever\n",
       "\n",
       "[10222 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lab= pd.read_csv('labels.csv')\n",
    "lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QP8YAzQvqyK-"
   },
   "source": [
    "### Print the count of each category of Dogs given in the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "3L2naXlr96Im",
    "outputId": "44acf0e3-c193-4086-9000-636e6c84ca79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scottish_deerhound      126\n",
       "maltese_dog             117\n",
       "afghan_hound            116\n",
       "entlebucher             115\n",
       "bernese_mountain_dog    114\n",
       "                       ... \n",
       "golden_retriever         67\n",
       "komondor                 67\n",
       "brabancon_griffon        67\n",
       "eskimo_dog               66\n",
       "briard                   66\n",
       "Name: breed, Length: 120, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of each category of Dogs in the dataset\n",
    "lab['breed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WI94_Qcc0D4M"
   },
   "source": [
    "### Get one-hot encodings of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q48iAcY196I3"
   },
   "outputs": [],
   "source": [
    "# Encode the different labels for Breed class\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder() \n",
    "y_encode = enc.fit_transform(lab['breed'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9nlWmRNM96I8",
    "outputId": "6fe53b35-7556-4e21-9f48-395be7843967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 0\n"
     ]
    }
   ],
   "source": [
    "# As can be seen below, 120 different breeds are labelled with values from 0 to 119\n",
    "print (max(y_encode),  min(y_encode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWaJ9naXfoiU"
   },
   "source": [
    "## Preparing training dataset\n",
    "1. Write a code which reads each and every id from labels.csv file and loads the corresponding image (in RGB - 128, 128, 3) from the train folder. <br>\n",
    "2. Create 2 variables <br> \n",
    "     a.  x_train - Should have all the images of the dogs from train folder <br>\n",
    "     b.  y_train - Corresponding label of the dog <br>\n",
    "<u>Note:</u> The id of the dog images and its corresponding labels are available in labels.csv file   \n",
    "<u>Hint:</u> Watch the video shared on \"Preparing the training dataset\" if you face issue on creating the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "aC2f9ecR0XGR",
    "outputId": "2bc5a94b-e1dc-4016-f0ed-8c19104b0c20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [00:30<00:00, 334.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train set\n",
    "\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "img_rows= 128\n",
    "img_cols = 128\n",
    "\n",
    "for f, img in tqdm(lab.values):\n",
    "  train_img= cv2.imread('./train/{}.jpg'.format(f),1)\n",
    "  train_img_resize = cv2.resize(train_img, (img_rows, img_cols))\n",
    "  X_train.append(train_img_resize)\n",
    "  Y_train.append(img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "colab_type": "code",
    "id": "Va_byNvt1UfK",
    "outputId": "05bc570a-3562-4006-8ef1-0d97bf61447c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 69, 116, 173],\n",
       "        [ 38,  91, 136],\n",
       "        [102, 166, 204],\n",
       "        ...,\n",
       "        [130, 204, 237],\n",
       "        [ 85, 193, 220],\n",
       "        [ 59, 138, 170]],\n",
       "\n",
       "       [[ 42,  77, 118],\n",
       "        [142, 180, 216],\n",
       "        [119, 162, 197],\n",
       "        ...,\n",
       "        [ 80, 152, 194],\n",
       "        [101, 183, 218],\n",
       "        [ 74, 177, 212]],\n",
       "\n",
       "       [[ 65, 107, 151],\n",
       "        [ 57, 103, 147],\n",
       "        [ 66, 107, 153],\n",
       "        ...,\n",
       "        [ 37, 121, 172],\n",
       "        [ 73, 145, 183],\n",
       "        [ 64, 168, 207]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 77, 119, 190],\n",
       "        [ 66, 121, 186],\n",
       "        [ 66, 122, 183],\n",
       "        ...,\n",
       "        [ 70,  91, 159],\n",
       "        [ 59,  89, 154],\n",
       "        [ 91, 132, 184]],\n",
       "\n",
       "       [[ 65, 118, 185],\n",
       "        [ 72, 124, 190],\n",
       "        [ 74, 125, 187],\n",
       "        ...,\n",
       "        [ 54,  63, 132],\n",
       "        [ 74, 116, 169],\n",
       "        [121, 169, 223]],\n",
       "\n",
       "       [[ 63, 116, 183],\n",
       "        [ 73, 125, 191],\n",
       "        [ 76, 127, 190],\n",
       "        ...,\n",
       "        [ 50,  51, 127],\n",
       "        [ 31,  62, 128],\n",
       "        [ 90, 145, 198]]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nkkZEpOe0ipk",
    "outputId": "b47bb16d-102d-4b5f-da79-08fd23feb1d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boston_bull'"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hya10mRN3Rog",
    "outputId": "4af43c3c-6632-4bd4-d0d4-27af944dbb9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 128, 128, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "v277JrIrobmi",
    "outputId": "f45be72e-be90-452b-9007-d2ed2fbfb2c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 120)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ioWDEgElBOs"
   },
   "source": [
    "Normalize the training data and convert into 4 dimensions so that it can be used as an input to conv layers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ARn76j3U1CDa"
   },
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "\n",
    "# Encoding the class variable\n",
    "\n",
    "import pandas as pd\n",
    "Y_train = pd.get_dummies(Y_train)\n",
    "\n",
    "# Converting X and Y sets to Arrays\n",
    "import numpy as np\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "\n",
    "# Reshaping the X set into 4 dimensions\n",
    "# convert from int to float\n",
    "X_train = X_train.reshape(X_train.shape[0],128,128,3).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdCXuAE11gZL"
   },
   "source": [
    "### Split the training and validation data from `x_train_data` and `y_train_data` obtained from above step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpWx-pgV96Jv"
   },
   "outputs": [],
   "source": [
    "# Splitting the given data into x_train_data and y_train_data set in the ratio of 7:3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train_data, x_val, y_train_data, y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkL-N1jDsU8m"
   },
   "source": [
    "### Loading the test data\n",
    "Read the id column from the samples_submission.csv and store it in test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "DnpXdpd9b3E7",
    "outputId": "0b910b96-08a6-487e-b264-c56447e8cf74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        000621fb3cbb32d8935728e48679680e\n",
       "1        00102ee9d8eb90812350685311fe5890\n",
       "2        0012a730dfa437f5f3613fb75efcd4ce\n",
       "3        001510bc8570bbeee98c8d80c8a95ec1\n",
       "4        001a5f3114548acdefa3d4da05474c2e\n",
       "                       ...               \n",
       "10352    ffeda8623d4eee33c6d1156a2ecbfcf8\n",
       "10353    fff1ec9e6e413275984966f745a313b0\n",
       "10354    fff74b59b758bbbf13a5793182a9bbe4\n",
       "10355    fff7d50d848e8014ac1e9172dc6762a3\n",
       "10356    fffbff22c1f51e3dc80c4bf04089545b\n",
       "Name: id, Length: 10357, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from samples_submission.csv\n",
    "\n",
    "test= pd.read_csv('sample_submission.csv')\n",
    "test_img= test['id']\n",
    "test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEJqZIMbm0Jo"
   },
   "source": [
    "Run the below code to load the test image files in x_test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zf7n4WG-b3Hv",
    "outputId": "a516c7f7-643b-4c0f-82b5-1b2fad02a3a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10357/10357 [00:32<00:00, 317.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the Test feature from the given file\n",
    "\n",
    "x_test_feature = []\n",
    "i = 0 # initialisation\n",
    "for f in tqdm(test_img.values): # f for format ,jpg\n",
    "    img = cv2.imread('./test/{}.jpg'.format(f), 1)\n",
    "    img_resize = cv2.resize(img, (img_rows, img_cols)) \n",
    "    x_test_feature.append(img_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3lHj7bMh6zyT",
    "outputId": "7bfd98cd-fc82-4475-8a1a-7d23c6c9aa39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_feature[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "LUtUVyWl9pCk",
    "outputId": "79aab788-92d7-4eb5-d68e-f63c750b9e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7155, 128, 128, 3)\n",
      "(3067, 128, 128, 3)\n",
      "(7155, 120)\n",
      "(3067, 120)\n"
     ]
    }
   ],
   "source": [
    "# Printing the shapes of train and validation sets\n",
    "\n",
    "\n",
    "# convert from int to float\n",
    "x_train_data = x_train_data.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "\n",
    "#Normalizing the input\n",
    "x_train_data /= 255\n",
    "x_val /= 255\n",
    "\n",
    "print (x_train_data.shape)\n",
    "print (x_val.shape)\n",
    "print (y_train_data.shape)\n",
    "print (y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9My6qSyDnE-_"
   },
   "source": [
    "Normalize the test data and convert it into 4 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93n-IntMnJGI"
   },
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "\n",
    "import numpy as np\n",
    "x_test_feature= np.array(x_test_feature)\n",
    "\n",
    "x_test_feature = x_test_feature.reshape(x_test_feature.shape[0],128,128,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKezNJVMsocP"
   },
   "source": [
    "### Build a basic conv neural network with 2 conv layers (kernel sizes - 5 and 3) add layers as mentioned below for classification.\n",
    "\n",
    "1. Add a Dense layer with 256 neurons with `relu` activation\n",
    "\n",
    "2. Add a Dense layer with 120 neurons as final layer (as there are 120 classes in the given dataset) with `softmax` activation for classifiaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "D2jxTY2S96J4",
    "outputId": "2284cc12-72a3-43ab-8016-9dc6cefe77e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 5600 samples, validate on 2400 samples\n",
      "Epoch 1/10\n",
      "5600/5600 [==============================] - 14s 3ms/step - loss: 4.7711 - acc: 0.0170 - val_loss: 4.7305 - val_acc: 0.0229\n",
      "Epoch 2/10\n",
      "5600/5600 [==============================] - 7s 1ms/step - loss: 4.6436 - acc: 0.0277 - val_loss: 4.6641 - val_acc: 0.0271\n",
      "Epoch 3/10\n",
      "5600/5600 [==============================] - 7s 1ms/step - loss: 4.4891 - acc: 0.0527 - val_loss: 4.6473 - val_acc: 0.0213\n",
      "Epoch 4/10\n",
      "5600/5600 [==============================] - 7s 1ms/step - loss: 4.2966 - acc: 0.0771 - val_loss: 4.6483 - val_acc: 0.0271\n",
      "Epoch 5/10\n",
      "5600/5600 [==============================] - 7s 1ms/step - loss: 4.0721 - acc: 0.1118 - val_loss: 4.6531 - val_acc: 0.0308\n",
      "Epoch 6/10\n",
      "5600/5600 [==============================] - 7s 1ms/step - loss: 3.7833 - acc: 0.1573 - val_loss: 4.7972 - val_acc: 0.0329\n",
      "Epoch 7/10\n",
      "5600/5600 [==============================] - 7s 1ms/step - loss: 3.4552 - acc: 0.2304 - val_loss: 4.9714 - val_acc: 0.0333\n",
      "Epoch 8/10\n",
      "5600/5600 [==============================] - 7s 1ms/step - loss: 3.0613 - acc: 0.2979 - val_loss: 5.1330 - val_acc: 0.0329\n",
      "Epoch 9/10\n",
      "5600/5600 [==============================] - 7s 1ms/step - loss: 2.6140 - acc: 0.3961 - val_loss: 5.2272 - val_acc: 0.0346\n",
      "Epoch 10/10\n",
      "5600/5600 [==============================] - 7s 1ms/step - loss: 2.0816 - acc: 0.5275 - val_loss: 5.5503 - val_acc: 0.0358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f99602d2e48>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic CNN Network\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10, mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import adam\n",
    "\n",
    "# Initialize and Build the Model\n",
    "\n",
    "\n",
    "TRAIN = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Define the Type of Model\n",
    "model1 = Sequential()\n",
    "\n",
    "# 1st Conv Layer with Kernel size 5\n",
    "model1.add(BatchNormalization(input_shape = (128,128,3)))\n",
    "model1.add(Convolution2D(32, kernel_size=(5, 5), activation ='relu', input_shape = (128, 128, 3)))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "# 2nd Conv Layer with Kernel size 3\n",
    "model1.add(Convolution2D(32, 3, 3))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "# Fully Connected Dense Layer\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(256))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "# Final Dense Layer\n",
    "model1.add(Dense(120))\n",
    "model1.add(Activation('softmax'))\n",
    "\n",
    "# Loss and Optimizer\n",
    "from keras.optimizers import Adam, SGD\n",
    "opt=SGD(lr=0.001)\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "# Train the model\n",
    "model1.fit(x_train_data, y_train_data, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, \n",
    "              validation_data=(x_val, y_val))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "k5K5kS4sKBsk",
    "outputId": "19b7a45b-794a-4098-906a-4febaefbd17e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7155 samples, validate on 3067 samples\n",
      "Epoch 1/10\n",
      "7155/7155 [==============================] - 15s 2ms/step - loss: 4.7647 - acc: 0.0152 - val_loss: 4.7448 - val_acc: 0.0192\n",
      "Epoch 2/10\n",
      "7155/7155 [==============================] - 14s 2ms/step - loss: 4.6379 - acc: 0.0294 - val_loss: 4.6761 - val_acc: 0.0280\n",
      "Epoch 3/10\n",
      "7155/7155 [==============================] - 14s 2ms/step - loss: 4.4677 - acc: 0.0506 - val_loss: 4.6364 - val_acc: 0.0293\n",
      "Epoch 4/10\n",
      "7155/7155 [==============================] - 14s 2ms/step - loss: 4.2253 - acc: 0.0837 - val_loss: 4.5919 - val_acc: 0.0372\n",
      "Epoch 5/10\n",
      "7155/7155 [==============================] - 14s 2ms/step - loss: 3.9239 - acc: 0.1276 - val_loss: 4.6296 - val_acc: 0.0408\n",
      "Epoch 6/10\n",
      "7155/7155 [==============================] - 14s 2ms/step - loss: 3.4659 - acc: 0.2117 - val_loss: 4.7879 - val_acc: 0.0349\n",
      "Epoch 7/10\n",
      "7155/7155 [==============================] - 14s 2ms/step - loss: 2.8654 - acc: 0.3420 - val_loss: 5.0702 - val_acc: 0.0411\n",
      "Epoch 8/10\n",
      "7155/7155 [==============================] - 14s 2ms/step - loss: 1.8948 - acc: 0.5656 - val_loss: 5.6658 - val_acc: 0.0313\n",
      "Epoch 9/10\n",
      "7155/7155 [==============================] - 14s 2ms/step - loss: 0.5809 - acc: 0.8864 - val_loss: 6.1576 - val_acc: 0.0450\n",
      "Epoch 10/10\n",
      "7155/7155 [==============================] - 14s 2ms/step - loss: 0.0845 - acc: 0.9937 - val_loss: 6.7882 - val_acc: 0.0476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc4dddef28>"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic CNN Network with Batch size 128\n",
    "\n",
    "# Initialize and Build the Model\n",
    "\n",
    "\n",
    "TRAIN = False\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "\n",
    "# Define the Type of Model\n",
    "model1 = Sequential()\n",
    "\n",
    "# 1st Conv Layer with Kernel size 5\n",
    "model1.add(BatchNormalization(input_shape = (128,128,3)))\n",
    "model1.add(Convolution2D(32, kernel_size=(5, 5), activation ='relu', input_shape = (128, 128, 3)))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "# 2nd Conv Layer with Kernel size 3\n",
    "model1.add(Convolution2D(32, 3, 3))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "# Fully Connected Dense Layer\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(256))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "# Final Dense Layer\n",
    "model1.add(Dense(120))\n",
    "model1.add(Activation('softmax'))\n",
    "\n",
    "# Loss and Optimizer\n",
    "opt=SGD(lr=0.001)\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "# Train the model\n",
    "model1.fit(x_train_data, y_train_data, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, \n",
    "              validation_data=(x_val, y_val))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ui8EXw6_oqpR"
   },
   "source": [
    "### Use batch_size = 128 and epochs = 10 and execute the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8hWaKmjoz69"
   },
   "source": [
    "#The model accuracy is very poor !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agJKkc6xtKiq"
   },
   "source": [
    "### Use Data Augmentation in the above model to see if the accuracy improves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "31Mn8qnZb3Ru"
   },
   "outputs": [],
   "source": [
    "# Improving the model through Data Augmentation\n",
    "# set data augmentation configuration\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        featurewise_center=False, samplewise_center=False,    featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  zca_whitening=False,  rotation_range=50,  \n",
    "        width_shift_range=0.1,  height_shift_range=0.1, horizontal_flip=True, \n",
    "        vertical_flip=False,shear_range=0.2,zoom_range=0.25,fill_mode='nearest')\n",
    "\n",
    "val_datagen = ImageDataGenerator(featurewise_center=False, samplewise_center=False,    featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  zca_whitening=False,  rotation_range=50,  \n",
    "        width_shift_range=0.1,  height_shift_range=0.1, horizontal_flip=True, \n",
    "        vertical_flip=False,shear_range=0.2,zoom_range=0.25,fill_mode='nearest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sssbaTfxlkk"
   },
   "source": [
    "### Using the above objects, create the image generators with variable names `train_generator` and `val_generator`\n",
    "\n",
    "You need to use train_datagen.flow() and val_datagen.flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sehaRgT-96KQ"
   },
   "outputs": [],
   "source": [
    "# Train Generator\n",
    "train_generator = train_datagen.flow(x_train_data, y_train_data, batch_size=128)\n",
    "\n",
    "# Validation generator\n",
    "\n",
    "val_generator = val_datagen.flow(x_val, y_val, batch_size=128)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVFQJZw3x4-C"
   },
   "source": [
    "### Fit the model using fit_generator() using `train_generator` and `val_generator` from the above step with 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "colab_type": "code",
    "id": "J1K2MqHbuPUa",
    "outputId": "47fa99e7-748a-44a8-98cc-2c46b64340a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 12s 745ms/step - loss: 4.7634 - acc: 0.0147 - val_loss: 8.7239 - val_acc: 0.0089\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 11s 673ms/step - loss: 4.7613 - acc: 0.0198 - val_loss: 8.4155 - val_acc: 0.0089\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 11s 673ms/step - loss: 4.7616 - acc: 0.0191 - val_loss: 8.1303 - val_acc: 0.0089\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 11s 670ms/step - loss: 4.7582 - acc: 0.0191 - val_loss: 7.8370 - val_acc: 0.0089\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 11s 683ms/step - loss: 4.7531 - acc: 0.0198 - val_loss: 7.4631 - val_acc: 0.0089\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 11s 667ms/step - loss: 4.7515 - acc: 0.0219 - val_loss: 7.3098 - val_acc: 0.0089\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 11s 689ms/step - loss: 4.7517 - acc: 0.0225 - val_loss: 7.0228 - val_acc: 0.0089\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 10s 656ms/step - loss: 4.7480 - acc: 0.0193 - val_loss: 6.7848 - val_acc: 0.0089\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 11s 695ms/step - loss: 4.7448 - acc: 0.0298 - val_loss: 6.6112 - val_acc: 0.0089\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 11s 675ms/step - loss: 4.7424 - acc: 0.0191 - val_loss: 6.4582 - val_acc: 0.0089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4d2da823c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "model1.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=10,validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2zmLztqo5DY"
   },
   "source": [
    "# Model accuracy is still poor!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSTATrhsAo7L"
   },
   "source": [
    "### Lets use Transfer Learning\n",
    "\n",
    "Download the vgg wieght file from here : https://github.com/MinerKasch/applied_deep_learning/blob/master/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zy5JdbW6pIvD"
   },
   "source": [
    "Use the below code to load VGG16 weights trained on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrqs0zg7ApNw"
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "path='/content/drive/My Drive/'\n",
    "# Instantiate the model with the pre-trained weights (no top)\n",
    "base_model= VGG16(weights=(path+'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'),\n",
    "                 include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5RFiD-dvHkjJ"
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EItOlRBGpV_A"
   },
   "source": [
    "Print the summary of the base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "colab_type": "code",
    "id": "lQsEBgnlpHjH",
    "outputId": "01dfc52a-3ea8-45b5-97bc-53a4b888e0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of the base model\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHpeOyW0qauW"
   },
   "source": [
    "### Add the following classification layers to the imported VGG Model <br>\n",
    "1. Flatten Layer\n",
    "2. Dense layer with 1024 neurons with activation as Relu\n",
    "3. Dense layer with 256 neurons with activation as Relu\n",
    "4. Dense layer with 120 neurons with activation as Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaSHb_wHnfCz"
   },
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D\n",
    "x=base_model.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BpT4MLkqoaO"
   },
   "outputs": [],
   "source": [
    "x=Flatten()(x) #Flatten\n",
    "x=Dense(1024,activation='relu')(x) #dense layer 1\n",
    "x=Dense(256,activation='relu')(x) #dense layer 2\n",
    "predictions=Dense(120,activation='softmax')(x) #final dense layer with softmax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_6hArV0aiim"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "model=Model(inputs=base_model.input,outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LeQem0pHITIj"
   },
   "source": [
    "### Make all the layers in the base_model (VGG16) to be non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "C7w9CSPvIRnX",
    "outputId": "eae06b63-311e-4a5d-e0a8-9f27ae6dba98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 block1_conv1\n",
      "2 block1_conv2\n",
      "3 block1_pool\n",
      "4 block2_conv1\n",
      "5 block2_conv2\n",
      "6 block2_pool\n",
      "7 block3_conv1\n",
      "8 block3_conv2\n",
      "9 block3_conv3\n",
      "10 block3_pool\n",
      "11 block4_conv1\n",
      "12 block4_conv2\n",
      "13 block4_conv3\n",
      "14 block4_pool\n",
      "15 block5_conv1\n",
      "16 block5_conv2\n",
      "17 block5_conv3\n",
      "18 block5_pool\n",
      "19 global_average_pooling2d_2\n",
      "20 dense_6\n",
      "21 dense_7\n",
      "22 dense_8\n"
     ]
    }
   ],
   "source": [
    "# Understanding the layers to be trained and non-trained\n",
    "for i,layer in enumerate(model.layers):\n",
    "  print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjeXjZoZbMn8"
   },
   "outputs": [],
   "source": [
    "# Base models to be non-trainable and others to be trainable\n",
    "\n",
    "for layer in model.layers[:19]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[19:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kj-BwqgfIkdv"
   },
   "source": [
    "### Fit and compile the model with batch_size = 128 and epochs = 10 and execute the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YD5fAgVQIpKZ"
   },
   "source": [
    "Try to get training and validation accuracy to be more than 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "colab_type": "code",
    "id": "SZk2SWvjIoRP",
    "outputId": "fa2e073b-457f-41b0-bb50-16e2367f53a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43/43 [==============================] - 28s 657ms/step - loss: 13.8050 - acc: 0.1369 - val_loss: 14.0985 - val_acc: 0.1154\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 28s 643ms/step - loss: 13.4741 - acc: 0.1562 - val_loss: 13.9132 - val_acc: 0.1296\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 27s 637ms/step - loss: 13.0743 - acc: 0.1777 - val_loss: 13.6686 - val_acc: 0.1425\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 28s 641ms/step - loss: 12.8952 - acc: 0.1894 - val_loss: 13.6136 - val_acc: 0.1475\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 27s 636ms/step - loss: 12.7590 - acc: 0.1993 - val_loss: 13.4249 - val_acc: 0.1583\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 27s 636ms/step - loss: 12.5520 - acc: 0.2109 - val_loss: 13.5291 - val_acc: 0.1529\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 28s 641ms/step - loss: 12.3615 - acc: 0.2231 - val_loss: 13.4153 - val_acc: 0.1621\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 27s 629ms/step - loss: 12.2642 - acc: 0.2266 - val_loss: 13.3129 - val_acc: 0.1658\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 27s 624ms/step - loss: 12.2236 - acc: 0.2309 - val_loss: 13.1828 - val_acc: 0.1708\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 26s 615ms/step - loss: 12.0263 - acc: 0.2458 - val_loss: 13.2200 - val_acc: 0.1721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f97c612bdd8>"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the model on the train and validation set\n",
    "from keras.optimizers import RMSprop\n",
    "opt=Adam(lr=0.01)\n",
    "#opt=RMSprop(lr=0.01, rho=0.9)\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=10,validation_data=val_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 967
    },
    "colab_type": "code",
    "id": "nEYzUd50h4FV",
    "outputId": "06e5b191-e311-47bc-c5c0-9a6a9e2d7277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 120)               30840     \n",
      "=================================================================\n",
      "Total params: 15,533,240\n",
      "Trainable params: 818,552\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "R7_CNN_Project2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
